{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"validation\" \"train\" \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/yuxiong/Desktop/Capstone/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanslate Class Descriptions\n",
    "#### Trainable Class: Handbag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_class_descriptions(trainable_classes_file, descriptions_file):\n",
    "    with open(trainable_classes_file, 'r') as file:\n",
    "        trainable_classes = file.read().strip()\n",
    "        #print(trainable_classes)\n",
    "    description_table = {}\n",
    "    with open(\"{}/classes/class-descriptions.csv\".format(base_dir)) as f:\n",
    "        #print(csv.reader(f))\n",
    "        for row in csv.reader(f):\n",
    "            if len(row):\n",
    "                description_table[row[0]] = row[1].replace(\"\\\"\", \"\").replace(\"'\", \"\").replace('`', '')\n",
    "        #print(description_table)\n",
    "    output = []\n",
    "\n",
    "    output.append(description_table[trainable_classes])\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_classes(formatted_data, translated_path):\n",
    "    with open(translated_path, 'w+') as f:\n",
    "        json.dump(formatted_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = translate_class_descriptions(\"{}/classes/classes-bbox-trainable.txt\".format(base_dir), \"{}/classes/class-descriptions.csv\".format(base_dir))\n",
    "\n",
    "save_classes(translated, \"{}/classes/trainable_translated.csv\".format(base_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1 Format Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_annotations(annotation_path, trainable_classes_path):\n",
    "    annotations = []\n",
    "    ids = []\n",
    "    with open(trainable_classes_path, 'r') as file:\n",
    "        trainable_classes = file.read().strip()\n",
    "        #print(type(trainable_classes))\n",
    "    with open(annotation_path, 'r') as annofile:\n",
    "        for row in csv.reader(annofile):\n",
    "            annotation = {'id': row[0], 'label': row[2], 'confidence': row[3], 'x0': row[4],\n",
    "                          'x1': row[5], 'y0': row[6], 'y1': row[7]}\n",
    "            #print(annotation[\"id\"])\n",
    "            #print(annotation)\n",
    "            if annotation['label'] == trainable_classes:\n",
    "            \n",
    "                annotations.append(annotation)\n",
    "                ids.append(row[0])\n",
    "    ids = dedupe(ids)\n",
    "    return annotations, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations, valid_image_ids = format_annotations(\"{}/bbox_annotations/{}/annotations-human-bbox.csv\".format(base_dir,dataset), \"{}/classes/classes-bbox-trainable.txt\".format(base_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence': '1',\n",
       " 'id': '3339a5c598981879',\n",
       " 'label': '/m/080hkjn',\n",
       " 'x0': '0.054879',\n",
       " 'x1': '0.945046',\n",
       " 'y0': '0.020680',\n",
       " 'y1': '0.961674'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2 Format Images URL csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_images(images_path):\n",
    "    images = []\n",
    "    with open(images_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        dataset = list(reader)\n",
    "        for row in dataset:\n",
    "            image = {'id': row[0], 'url': row[2]}\n",
    "            images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = format_images(\"{}/images/{}/images.csv\".format(base_dir,dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0001eeaf4aed83f9',\n",
       " 'url': 'https://c2.staticflickr.com/6/5606/15611395595_f51465687d_o.jpg'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3 Filter Image URL by Trainable Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check each image and only keep it if it's ID has a bounding box annotation associated with it.\n",
    "def filter_images(dataset, ids):\n",
    "    output_list = []\n",
    "    unique_ids = set(ids)\n",
    "    for element in tqdm(dataset, desc=\"filtering out non-essential images\"):\n",
    "        if element['id'] in unique_ids:\n",
    "            output_list.append(element)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def save_data(data, out_path):\n",
    "    with open(out_path, 'w+') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "# Gathers annotations for each image id, to be easier to work with.\n",
    "def points_maker(annotations):\n",
    "    by_id = {}\n",
    "    \n",
    "    for anno in tqdm(annotations, desc=\"grouping annotations\"):\n",
    "        #print(anno)\n",
    "        if anno['id'] in by_id:\n",
    "            by_id[anno['id']].append(anno)\n",
    "        else:\n",
    "            by_id[anno['id']] = []\n",
    "            by_id[anno['id']].append(anno)\n",
    "    groups = []\n",
    "    while len(by_id) >= 1:\n",
    "        key, value = by_id.popitem()\n",
    "        groups.append({'id': key, 'annotations': value})\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grouping annotations: 100%|██████████| 60/60 [00:00<00:00, 143886.93it/s]\n"
     ]
    }
   ],
   "source": [
    "points = points_maker(annotations)\n",
    "#filtered_images = filter_images(images, valid_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotations': [{'confidence': '1',\n",
       "   'id': '368f0045b95affc3',\n",
       "   'label': '/m/080hkjn',\n",
       "   'x0': '0.000049',\n",
       "   'x1': '1.000000',\n",
       "   'y0': '0.000000',\n",
       "   'y1': '0.999843'}],\n",
       " 'id': '368f0045b95affc3'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering out non-essential images: 100%|██████████| 41621/41621 [00:00<00:00, 1081116.51it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_images = filter_images(images, valid_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0040009ad56c2bc2',\n",
       " 'url': 'https://farm3.staticflickr.com/5174/5405309020_7ce65b0636_o.jpg'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(filtered_images, \"{}/filtered_images.json\".format(base_dir))\n",
    "save_data(points,\"{}/points.json\".format(base_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4. Download Handbag Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import argparse\n",
    "# Downloads all image files contained in dataset, if an image fails to download lets skip it.\n",
    "\n",
    "\n",
    "# This is a nice parallel processing tool that uses tqdm\n",
    "# to help visualize time-to-completion.\n",
    "def parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=3):\n",
    "    \"\"\"\n",
    "        A parallel version of the map function with a progress bar.\n",
    "        Args:\n",
    "            array (array-like): An array to iterate over.\n",
    "            function (function): A python function to apply to the elements of array\n",
    "            n_jobs (int, default=16): The number of cores to use\n",
    "            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of\n",
    "                keyword arguments to function\n",
    "            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job.\n",
    "                Useful for catching bugs\n",
    "        Returns:\n",
    "            [function(array[0]), function(array[1]), ...]\n",
    "    \"\"\"\n",
    "    #We run the first few iterations serially to catch bugs\n",
    "    if front_num > 0:\n",
    "        front = [function(**a) if use_kwargs else function(a) for a in array[:front_num]]\n",
    "    #If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n",
    "    if n_jobs==1:\n",
    "        return front + [function(**a) if use_kwargs else function(a) for a in tqdm(array[front_num:])]\n",
    "    #Assemble the workers\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as pool:\n",
    "        #Pass the elements of array into function\n",
    "        if use_kwargs:\n",
    "            futures = [pool.submit(function, **a) for a in array[front_num:]]\n",
    "        else:\n",
    "            futures = [pool.submit(function, a) for a in array[front_num:]]\n",
    "        kwargs = {\n",
    "            'total': len(futures),\n",
    "            'unit': 'it',\n",
    "            'unit_scale': True,\n",
    "            'leave': True\n",
    "        }\n",
    "        #Print out the progress as tasks complete\n",
    "        for f in tqdm(as_completed(futures), **kwargs):\n",
    "            pass\n",
    "    out = []\n",
    "    #Get the results from the futures.\n",
    "    for i, future in tqdm(enumerate(futures)):\n",
    "        try:\n",
    "            out.append(future.result())\n",
    "        except Exception as e:\n",
    "            out.append(e)\n",
    "    return front + out\n",
    "\n",
    "\n",
    "def download(element):\n",
    "    image_content = None\n",
    "    dir_path = \"{}/images/{}\".format(base_dir,dataset)\n",
    "    browser_headers = [\n",
    "        {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704 Safari/537.36\"},\n",
    "        {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743 Safari/537.36\"},\n",
    "        {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:44.0) Gecko/20100101 Firefox/44.0\"}\n",
    "    ]\n",
    "    try:\n",
    "        response = requests.get(element['url'],\n",
    "                                headers=random.choice(browser_headers),\n",
    "                                verify=False)\n",
    "        image_content = response.content\n",
    "    except:\n",
    "        pass\n",
    "    if image_content:\n",
    "        complete_file_path = os.path.join(dir_path, element['id']+'.'+element['url'].split('.')[-1])\n",
    "        with open(complete_file_path, \"wb\") as f:\n",
    "            f.write(image_content)\n",
    "            f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--images_path', dest='images_path', required=True)\n",
    "parser.add_argument('--images_output_directory', dest='images_output_directory', required=True)\n",
    "\n",
    "\n",
    "browser_headers = [\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704 Safari/537.36\"},\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743 Safari/537.36\"},\n",
    "    {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:44.0) Gecko/20100101 Firefox/44.0\"}\n",
    "]\n",
    "try:\n",
    "    os.makedirs(\"{}/images/{}\".format(base_dir,dataset))\n",
    "except OSError:\n",
    "    pass  # already exists\n",
    "with open(\"{}/filtered_images.json\".format(base_dir), 'r') as f:\n",
    "    image_urls = json.load(f)\n",
    "parallel_process(image_urls, download)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5. Process_Images: Image Verification and Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(saved_images_path, resized_images_path, points):\n",
    "    cleaned_points = []\n",
    "    for point in tqdm(points, desc=\"checking if images are valid from label index\"):\n",
    "        \n",
    "        stored_path = os.path.join(saved_images_path, point['id'] + '.jpg')\n",
    "\n",
    "        im = Image.open(stored_path)\n",
    "\n",
    "        \n",
    "       \n",
    "        im.thumbnail((256, 256))\n",
    "\n",
    "        if resized_images_path:\n",
    "            resized_path = os.path.join(resized_images_path, point['id'] + '.jpg')\n",
    "            \n",
    "            im.save(resized_path)\n",
    "        else:\n",
    "            os.remove(stored_path)\n",
    "            im.save(stored_path)\n",
    "        cleaned_points.append(point)\n",
    "\n",
    "    return cleaned_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checking if images are valid from label index: 100%|██████████| 55/55 [00:01<00:00, 51.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def save_dataset(data, file_path):\n",
    "    with open(file_path, 'w+') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "points = load_dataset(\"{}/points.json\".format(base_dir))\n",
    "filtered_points = process_images(\"{}/images/{}\".format(base_dir,dataset), \"{}/images/{}\".format(base_dir,dataset), points)\n",
    "save_dataset(filtered_points, \"{}/points.json\".format(base_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
