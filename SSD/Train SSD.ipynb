{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import xml.etree.ElementTree as ET  # parse xml file\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VOC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = (  # always index 0\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDetection(data.Dataset):\n",
    "    \"\"\"VOC Detection Dataset Object\n",
    "\n",
    "    input is image, target is annotation\n",
    "\n",
    "    Arguments:\n",
    "        root (string): filepath to VOCdevkit folder.\n",
    "        image_set (list with tuple-string): imageset to use (eg. [('2007', 'train')])\n",
    "        transform (callable, optional): transformation to perform on the input image\n",
    "        target_transform (callable, optional): transformation to perform on the target `annotation`\n",
    "            (eg: take in caption string, return tensor of word indices)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, image_set, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.image_set = image_set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self._annopath = os.path.join('%s', 'Annotations', '%s.xml')\n",
    "        self._imgpath = os.path.join('%s', 'JPEGImages', '%s.jpg')\n",
    "        self.ids = list()\n",
    "        for (year, name) in image_set:\n",
    "            rootpath = os.path.join(self.root, 'VOC' + year)\n",
    "            for line in open(os.path.join(rootpath, 'ImageSets', 'Main', name + '.txt')):\n",
    "                self.ids.append((rootpath, line.strip()))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img, gt, h, w = self.pull_item(item)\n",
    "        return img, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        target = ET.parse(self._annopath % img_id).getroot()\n",
    "        img = cv2.imread(self._imgpath % img_id)\n",
    "        height, width, channel = img.shape\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target, width, height)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            target = np.array(target)\n",
    "            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n",
    "            img = img[:, :, (2, 1, 0)]  # bgr->rgb\n",
    "            target = np.c_[boxes, np.expand_dims(labels, axis=1)]\n",
    "\n",
    "        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n",
    "\n",
    "    def pull_image(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        # Note: here use the bgr form (rgb is also do well: remember to change mean)\n",
    "        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n",
    "\n",
    "    def pull_anno(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        anno = ET.parse(self._annopath % img_id).getroot()\n",
    "        gt = self.target_transform(anno, 1, 1)  # back original size\n",
    "        return img_id[1], gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationTransform(object):\n",
    "    \"\"\"Transforms a VOC annotation into a Tensor of bbox coords and label index\n",
    "    Initilized with a dictionary lookup of classnames to indexes\n",
    "\n",
    "    Arguments:\n",
    "        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_to_ind=None, keep_difficult=False):\n",
    "        self.class_to_ind = class_to_ind or dict(zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "    def __call__(self, target, width, height):\n",
    "        res = []\n",
    "        for obj in target.iter('object'):\n",
    "            difficult = int(obj.find('difficult').text) == 1\n",
    "            if not self.keep_difficult and difficult:\n",
    "                continue\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bbox = obj.find('bndbox')\n",
    "\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            bndbox = []\n",
    "            for i, pt in enumerate(pts):\n",
    "                cur_pt = int(bbox.find(pt).text) - 1\n",
    "                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n",
    "                bndbox.append(cur_pt)\n",
    "            label_idx = self.class_to_ind[name]\n",
    "            bndbox.append(label_idx)\n",
    "            res += [bndbox]  # each elem: [xmin, ymin, xmax, ymax, label_ind]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic transform: norm+scale, mean is bgr form\n",
    "# Note: weights from yolo-official not minus mean but with scale\n",
    "class BaseTransform(object):\n",
    "    def __init__(self, size=300, mean=(104, 117, 123), scale=False):\n",
    "        self.size = size\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        image = cv2.resize(image, (self.size, self.size)).astype(np.float32)\n",
    "        image -= self.mean\n",
    "        image = image / 255.0 if self.scale else image\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "def detection_collate(batch):\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets\n",
    "\n",
    "\n",
    "# image: np.array, box: tuple (left, top, right, bottom)\n",
    "def draw_box(image, label, box, c):\n",
    "    h, w = image.shape[:2]\n",
    "    thickness = (w + h) // 300\n",
    "    left, top, right, bottom = box\n",
    "    top, left = max(0, np.round(top).astype('int32')), max(0, np.round(left).astype('int32'))\n",
    "    right, bottom = min(w, np.round(right).astype('int32')), min(h, np.round(bottom).astype('int32'))\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), cfg.colors[c], thickness)\n",
    "    cv2.putText(image, label, (left, top - 5), 0, 0.5, cfg.colors[c], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = AnnotationTransform()\n",
    "root = '/scratch/rw2268/VOCdevkit/'\n",
    "image_set = [('2007', 'trainval')]\n",
    "dataset = VOCDetection(root, image_set, transform=BaseTransform(), target_transform=target_transform)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, BATCH_SIZE, num_workers=4,\n",
    "                         shuffle=True, collate_fn=detection_collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  89.,   90.,   90.,  ..., -123., -120., -112.],\n",
       "          [  89.,   90.,   90.,  ..., -123., -120., -112.],\n",
       "          [  87.,   88.,   89.,  ..., -123., -121., -106.],\n",
       "          ...,\n",
       "          [-103., -108., -108.,  ..., -123., -123., -123.],\n",
       "          [-104., -105., -110.,  ..., -122., -123., -122.],\n",
       "          [-107., -103., -108.,  ..., -122., -123., -122.]],\n",
       " \n",
       "         [[  97.,   98.,   99.,  ..., -117., -114., -105.],\n",
       "          [  98.,   99.,  100.,  ..., -117., -114., -105.],\n",
       "          [  99.,  100.,  100.,  ..., -117., -115.,  -99.],\n",
       "          ...,\n",
       "          [ -85.,  -91.,  -91.,  ..., -117., -117., -117.],\n",
       "          [ -88.,  -89.,  -94.,  ..., -117., -117., -116.],\n",
       "          [ -94.,  -90.,  -92.,  ..., -117., -117., -116.]],\n",
       " \n",
       "         [[ 131.,  132.,  132.,  ..., -104.,  -99.,  -88.],\n",
       "          [ 130.,  130.,  131.,  ..., -104.,  -99.,  -88.],\n",
       "          [ 127.,  129.,  131.,  ..., -104., -102.,  -85.],\n",
       "          ...,\n",
       "          [ -79.,  -85.,  -88.,  ..., -104., -104., -104.],\n",
       "          [ -85.,  -85.,  -91.,  ..., -104., -104., -103.],\n",
       "          [ -89.,  -84.,  -87.,  ..., -104., -104., -103.]]]),\n",
       " array([[ 0.39      ,  0.48955224,  0.976     ,  0.73432836, 18.        ]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5011\n",
      "torch.Size([3, 300, 300])\n",
      "[[0.524      0.56       0.646      0.90133333 8.        ]\n",
      " [0.328      0.70133333 0.504      0.98933333 8.        ]\n",
      " [0.48       0.51466667 0.588      0.79466667 8.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "img, gt = dataset[0]\n",
    "print(img.size())\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SSD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rw2268/1006/detection-pytorch/ssd/utils_ssd/L2Norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight, self.gamma)\n"
     ]
    }
   ],
   "source": [
    "from ssd.ssd300 import build_ssd\n",
    "net = build_ssd('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_() if m.bias is not None else None\n",
    "\n",
    "\n",
    "# Sets the learning rate to the initial LR decayed by 10 at every specified step\n",
    "def adjust_learning_rate(optimizer, lr, gamma, step):\n",
    "    lr = lr * (gamma ** step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "cuda = True\n",
    "lr = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(VOC_CLASSES) + 1\n",
    "overlap_thresh = 0.5\n",
    "neg_pos = 3\n",
    "variance = [0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd.utils_ssd.box_utils import match, log_sum_exp\n",
    "\n",
    "\n",
    "# evaluate conf_loss and loc_loss\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.variance = variance\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        loc_data, conf_data, priors = preds\n",
    "        num = loc_data.size(0)\n",
    "        num_priors = priors.size(0)\n",
    "        # match priors (priors->nearest target)\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        if loc_data.is_cuda:\n",
    "            loc_t, conf_t = loc_t.cuda(), conf_t.cuda()\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1]\n",
    "            labels = targets[idx][:, -1]\n",
    "            defaults = priors\n",
    "            match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx)\n",
    "        pos = conf_t > 0\n",
    "        # location loss\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "\n",
    "        # evaluate each priors's loss (the same as the paper)\n",
    "        batch_conf = conf_data\n",
    "        loss_c = (log_sum_exp(batch_conf) - batch_conf.gather(2, conf_t.unsqueeze(2))).squeeze(2)\n",
    "        # hard negative mining: note: the batch size of each iteration is not the same\n",
    "        # find the \"max loss\" background\n",
    "        loss_c[pos] = 0  # filter out pos boxes\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio * num_pos, max=pos.size(1) - 1)  # size: [num, 1]\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "        # confidence loss (pos:neg=1:3)\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx + neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weightd = conf_t[(pos + neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weightd, size_average=False)\n",
    "\n",
    "        return loss_l / num_pos.sum().float(), loss_c / num_pos.sum().float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = MultiBoxLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "loc_loss, conf_loss = 0, 0\n",
    "epoch_num = 3\n",
    "step_index = 0\n",
    "epoch_size = len(dataset) // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SSD on VOC\n",
      "Timer: 0.9458 sec.\n",
      "epoch 0 iter 0 || Loss: 29.2489 ||  Timer: 0.8835 sec.\n",
      "epoch 0 iter 10 || Loss: 17.5946 ||  Timer: 0.8857 sec.\n",
      "epoch 0 iter 20 || Loss: 15.4053 ||  Timer: 0.8900 sec.\n",
      "epoch 0 iter 30 || Loss: 15.7195 ||  Timer: 0.8901 sec.\n",
      "epoch 0 iter 40 || Loss: 15.0775 ||  Timer: 0.8977 sec.\n",
      "epoch 0 iter 50 || Loss: 15.1548 ||  Timer: 0.8855 sec.\n",
      "epoch 0 iter 60 || Loss: 14.6714 ||  Timer: 0.8783 sec.\n",
      "epoch 0 iter 70 || Loss: 14.7855 ||  Timer: 0.8893 sec.\n",
      "epoch 0 iter 80 || Loss: 14.7947 ||  Timer: 0.8953 sec.\n",
      "epoch 0 iter 90 || Loss: 14.9965 ||  Timer: 0.8944 sec.\n",
      "epoch 0 iter 100 || Loss: 14.7506 ||  Timer: 0.8917 sec.\n",
      "epoch 0 iter 110 || Loss: 14.6937 ||  Timer: 0.8882 sec.\n",
      "epoch 0 iter 120 || Loss: 14.6921 ||  Timer: 0.8919 sec.\n",
      "epoch 0 iter 130 || Loss: 14.8242 ||  Timer: 0.8911 sec.\n",
      "epoch 0 iter 140 || Loss: 14.7320 ||  Timer: 0.8840 sec.\n",
      "epoch 0 iter 150 || Loss: 14.7143 ||  Timer: 0.9341 sec.\n",
      "epoch 1 iter 0 || Loss: 14.6691 ||  Timer: 0.8928 sec.\n",
      "epoch 1 iter 10 || Loss: 14.8360 ||  Timer: 0.8930 sec.\n",
      "epoch 1 iter 20 || Loss: 14.6420 ||  Timer: 0.8827 sec.\n",
      "epoch 1 iter 30 || Loss: 14.7026 ||  Timer: 0.8803 sec.\n",
      "epoch 1 iter 40 || Loss: 14.5929 ||  Timer: 0.8901 sec.\n",
      "epoch 1 iter 50 || Loss: 14.7387 ||  Timer: 0.8935 sec.\n",
      "epoch 1 iter 60 || Loss: 14.6458 ||  Timer: 0.8948 sec.\n",
      "epoch 1 iter 70 || Loss: 14.6183 ||  Timer: 0.8858 sec.\n",
      "epoch 1 iter 80 || Loss: 14.7323 ||  Timer: 0.8958 sec.\n",
      "epoch 1 iter 90 || Loss: 14.7482 ||  Timer: 0.8924 sec.\n",
      "epoch 1 iter 100 || Loss: 14.6944 ||  Timer: 0.8935 sec.\n",
      "epoch 1 iter 110 || Loss: 14.7852 ||  Timer: 0.8815 sec.\n",
      "epoch 1 iter 120 || Loss: 14.5316 ||  Timer: 0.8857 sec.\n",
      "epoch 1 iter 130 || Loss: 14.6521 ||  Timer: 0.8942 sec.\n",
      "epoch 1 iter 140 || Loss: 14.4747 ||  Timer: 0.8847 sec.\n",
      "epoch 1 iter 150 || Loss: 14.6315 ||  Timer: 0.9141 sec.\n",
      "epoch 2 iter 0 || Loss: 14.6010 ||  Timer: 0.9129 sec.\n",
      "epoch 2 iter 10 || Loss: 14.6773 ||  Timer: 0.8959 sec.\n",
      "epoch 2 iter 20 || Loss: 14.6970 ||  Timer: 0.8966 sec.\n",
      "epoch 2 iter 30 || Loss: 14.5722 ||  Timer: 0.8988 sec.\n",
      "epoch 2 iter 40 || Loss: 14.6163 ||  Timer: 0.8901 sec.\n",
      "epoch 2 iter 50 || Loss: 14.4846 ||  Timer: 0.8932 sec.\n",
      "epoch 2 iter 60 || Loss: 14.5942 ||  Timer: 0.8866 sec.\n",
      "epoch 2 iter 70 || Loss: 14.5877 ||  Timer: 0.8911 sec.\n",
      "epoch 2 iter 80 || Loss: 14.5232 ||  Timer: 0.8929 sec.\n",
      "epoch 2 iter 90 || Loss: 14.5868 ||  Timer: 0.8873 sec.\n",
      "epoch 2 iter 100 || Loss: 14.6097 ||  Timer: 0.8854 sec.\n",
      "epoch 2 iter 110 || Loss: 14.7312 ||  Timer: 0.8957 sec.\n",
      "epoch 2 iter 120 || Loss: 14.6344 ||  Timer: 0.8918 sec.\n",
      "epoch 2 iter 130 || Loss: 14.5520 ||  Timer: 0.8864 sec.\n",
      "epoch 2 iter 140 || Loss: 14.7071 ||  Timer: 0.8839 sec.\n",
      "epoch 2 iter 150 || Loss: 14.5572 ||  "
     ]
    }
   ],
   "source": [
    "print(\"Training SSD on VOC\")\n",
    "\n",
    "batch_iterator = None\n",
    "images = torch.randn((BATCH_SIZE, 3, 300, 300), requires_grad=True)\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "    images = images.cuda()\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    # to do: adjust learning rate\n",
    "    for i, (imgs, targets) in enumerate(data_loader):\n",
    "        if i == epoch_size:\n",
    "            break\n",
    "        images.data.copy_(imgs)\n",
    "        targets = [anno.cuda() for anno in targets] if cuda else [anno for anno in targets]\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_c + loss_l\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        if i % 10 == 0:\n",
    "            print('Timer: %.4f sec.' % (t1 - t0))\n",
    "            print('epoch ' + repr(epoch) + ' iter ' + repr(i) + ' || Loss: %.4f || ' % (loss.item()), end=' ')\n",
    "#     if epoch % 20 == 0 and epoch != 0:\n",
    "#         print('Saving state, epoch: ', epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), '/scratch/rw2268/VOCresults/ssd/ssd300_' + repr(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if resume, load pre-trained model\n",
    "net = build_ssd('test', bone='vgg')\n",
    "net.load_state_dict(torch.load('/scratch/rw2268/VOCresults/ssd/ssd300_2.pth'))\n",
    "net.eval()\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "testset = VOCDetection(root, [('2007', 'test')], BaseTransform(), AnnotationTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self):\n",
    "        self.total_time = 0.\n",
    "        self.calls = 0\n",
    "        self.start_time = 0.\n",
    "        self.diff = 0.\n",
    "        self.average_time = 0.\n",
    "\n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def toc(self, avg=True):\n",
    "        self.diff = time.time() - self.start_time\n",
    "        self.total_time += self.diff\n",
    "        self.calls += 1\n",
    "        self.average_time = self.total_time / self.calls\n",
    "        if avg:\n",
    "            return self.average_time\n",
    "        else:\n",
    "            return self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----save the pred boxes+info to .pkl-----\n",
    "# all detections are collected into:\n",
    "# all_boxes[cls][image] = N x 5 array of detections in\n",
    "# (x1, y1, x2, y2, score)\n",
    "def generate_boxes(dataset, net, det_file):\n",
    "    img_num = len(dataset)\n",
    "    # TODO: delete\n",
    "    # img_num = 50\n",
    "    all_boxes = [[[] for _ in range(img_num)] for _ in range(num_classes)]\n",
    "\n",
    "    _t = {'im_detect': Timer(), 'misc': Timer()}\n",
    "\n",
    "    x = torch.randn((1, 3, 300, 300))\n",
    "    x = x.cuda() if cuda else x\n",
    "    for i in range(img_num):\n",
    "        im, gt, h, w = dataset.pull_item(i)\n",
    "        x.copy_(im.unsqueeze(0))\n",
    "        _t['im_detect'].tic()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y = net(x)\n",
    "        detect_time = _t['im_detect'].toc(avg=False)\n",
    "        # \"store\" to each class\n",
    "#         print(y[0].size())\n",
    "#         print(y[0])\n",
    "#         print(y[1].size())\n",
    "#         print(y[1])\n",
    "#         print(y[2].size())\n",
    "#         print(y[2])\n",
    "        for j in range(1, y.size(1)):\n",
    "            dets = y[0, j, :]\n",
    "            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n",
    "            dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "            if dets.size(0) == 0:\n",
    "                continue\n",
    "            boxes = dets[:, 1:]\n",
    "            boxes[:, 0::2] *= w\n",
    "            boxes[:, 1::2] *= h\n",
    "            scores = dets[:, 0].cpu().numpy()\n",
    "            cls_dets = np.c_[boxes.cpu().numpy(), scores]\n",
    "            all_boxes[j][i] = cls_dets\n",
    "        print('im_detect: {:d}/{:d} {:.3f}s'.format(i + 1, img_num, detect_time))\n",
    "        if i > 100:\n",
    "            break\n",
    "\n",
    "    with open(det_file, 'wb') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return all_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_dir(root, name):\n",
    "    filedir = os.path.join(root, name)\n",
    "    if not os.path.exists(filedir):\n",
    "        os.makedirs(filedir)\n",
    "    return filedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/scratch/rw2268/VOCresults/ssd'\n",
    "output_dir = get_output_dir(output_folder, 'eval')\n",
    "det_file = os.path.join(output_dir, 'detections.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict boxes, and save to .pkl\n",
      "im_detect: 1/4952 0.759s\n",
      "im_detect: 2/4952 0.766s\n",
      "im_detect: 3/4952 0.747s\n",
      "im_detect: 4/4952 0.746s\n",
      "im_detect: 5/4952 0.771s\n",
      "im_detect: 6/4952 0.755s\n",
      "im_detect: 7/4952 0.759s\n",
      "im_detect: 8/4952 0.756s\n",
      "im_detect: 9/4952 0.756s\n",
      "im_detect: 10/4952 0.759s\n",
      "im_detect: 11/4952 0.752s\n",
      "im_detect: 12/4952 0.752s\n",
      "im_detect: 13/4952 0.753s\n",
      "im_detect: 14/4952 0.749s\n",
      "im_detect: 15/4952 0.754s\n",
      "im_detect: 16/4952 0.763s\n",
      "im_detect: 17/4952 0.752s\n",
      "im_detect: 18/4952 0.763s\n",
      "im_detect: 19/4952 0.750s\n",
      "im_detect: 20/4952 0.757s\n",
      "im_detect: 21/4952 0.747s\n",
      "im_detect: 22/4952 0.750s\n",
      "im_detect: 23/4952 0.756s\n",
      "im_detect: 24/4952 0.765s\n",
      "im_detect: 25/4952 0.761s\n",
      "im_detect: 26/4952 0.761s\n",
      "im_detect: 27/4952 0.759s\n",
      "im_detect: 28/4952 0.757s\n",
      "im_detect: 29/4952 0.755s\n",
      "im_detect: 30/4952 0.758s\n",
      "im_detect: 31/4952 0.758s\n",
      "im_detect: 32/4952 0.755s\n",
      "im_detect: 33/4952 0.757s\n",
      "im_detect: 34/4952 0.756s\n",
      "im_detect: 35/4952 0.757s\n",
      "im_detect: 36/4952 0.744s\n",
      "im_detect: 37/4952 0.764s\n",
      "im_detect: 38/4952 0.771s\n",
      "im_detect: 39/4952 0.762s\n",
      "im_detect: 40/4952 0.763s\n",
      "im_detect: 41/4952 0.767s\n",
      "im_detect: 42/4952 0.755s\n",
      "im_detect: 43/4952 0.751s\n",
      "im_detect: 44/4952 0.754s\n",
      "im_detect: 45/4952 0.762s\n",
      "im_detect: 46/4952 0.758s\n",
      "im_detect: 47/4952 0.756s\n",
      "im_detect: 48/4952 0.749s\n",
      "im_detect: 49/4952 0.745s\n",
      "im_detect: 50/4952 0.762s\n",
      "im_detect: 51/4952 0.755s\n",
      "im_detect: 52/4952 0.760s\n",
      "im_detect: 53/4952 0.760s\n",
      "im_detect: 54/4952 0.762s\n",
      "im_detect: 55/4952 0.757s\n",
      "im_detect: 56/4952 0.753s\n",
      "im_detect: 57/4952 0.758s\n",
      "im_detect: 58/4952 0.760s\n",
      "im_detect: 59/4952 0.767s\n",
      "im_detect: 60/4952 0.759s\n",
      "im_detect: 61/4952 0.746s\n",
      "im_detect: 62/4952 0.761s\n",
      "im_detect: 63/4952 0.756s\n",
      "im_detect: 64/4952 0.759s\n",
      "im_detect: 65/4952 0.759s\n",
      "im_detect: 66/4952 0.776s\n",
      "im_detect: 67/4952 0.747s\n",
      "im_detect: 68/4952 0.762s\n",
      "im_detect: 69/4952 0.773s\n",
      "im_detect: 70/4952 0.759s\n",
      "im_detect: 71/4952 0.748s\n",
      "im_detect: 72/4952 0.756s\n",
      "im_detect: 73/4952 0.743s\n",
      "im_detect: 74/4952 0.764s\n",
      "im_detect: 75/4952 0.758s\n",
      "im_detect: 76/4952 0.757s\n",
      "im_detect: 77/4952 0.760s\n",
      "im_detect: 78/4952 0.763s\n",
      "im_detect: 79/4952 0.764s\n",
      "im_detect: 80/4952 0.749s\n",
      "im_detect: 81/4952 0.761s\n",
      "im_detect: 82/4952 0.762s\n",
      "im_detect: 83/4952 0.761s\n",
      "im_detect: 84/4952 0.767s\n",
      "im_detect: 85/4952 0.743s\n",
      "im_detect: 86/4952 0.749s\n",
      "im_detect: 87/4952 0.778s\n",
      "im_detect: 88/4952 0.771s\n",
      "im_detect: 89/4952 0.772s\n",
      "im_detect: 90/4952 0.762s\n",
      "im_detect: 91/4952 0.761s\n",
      "im_detect: 92/4952 0.770s\n",
      "im_detect: 93/4952 0.765s\n",
      "im_detect: 94/4952 0.758s\n",
      "im_detect: 95/4952 0.759s\n",
      "im_detect: 96/4952 0.759s\n",
      "im_detect: 97/4952 0.752s\n",
      "im_detect: 98/4952 0.755s\n",
      "im_detect: 99/4952 0.757s\n",
      "im_detect: 100/4952 0.756s\n",
      "im_detect: 101/4952 0.766s\n",
      "im_detect: 102/4952 0.755s\n"
     ]
    }
   ],
   "source": [
    "print('predict boxes, and save to .pkl')\n",
    "box_list = generate_boxes(testset, net, det_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(np.array([[1,2,3],[2,1,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-39000e73a69b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "a.gt(2).expand(4, a.size(0)).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'expand' of 'torch._C._TensorBase' object needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f551275eae57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'expand' of 'torch._C._TensorBase' object needs an argument"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getsource(torch.Tensor.expand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<method 'expand' of 'torch._C._TensorBase' objects>\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor.expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
