{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import xml.etree.ElementTree as ET  # parse xml file\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VOC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = (  # always index 0\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDetection(data.Dataset):\n",
    "    \"\"\"VOC Detection Dataset Object\n",
    "\n",
    "    input is image, target is annotation\n",
    "\n",
    "    Arguments:\n",
    "        root (string): filepath to VOCdevkit folder.\n",
    "        image_set (list with tuple-string): imageset to use (eg. [('2007', 'train')])\n",
    "        transform (callable, optional): transformation to perform on the input image\n",
    "        target_transform (callable, optional): transformation to perform on the target `annotation`\n",
    "            (eg: take in caption string, return tensor of word indices)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, image_set, transform=None, target_transform=None):\n",
    "        self.root = root\n",
    "        self.image_set = image_set\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self._annopath = os.path.join('%s', 'Annotations', '%s.xml')\n",
    "        self._imgpath = os.path.join('%s', 'JPEGImages', '%s.jpg')\n",
    "        self.ids = list()\n",
    "        for (year, name) in image_set:\n",
    "            rootpath = os.path.join(self.root, 'VOC' + year)\n",
    "            for line in open(os.path.join(rootpath, 'ImageSets', 'Main', name + '.txt')):\n",
    "                self.ids.append((rootpath, line.strip()))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img, gt, h, w = self.pull_item(item)\n",
    "        return img, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        target = ET.parse(self._annopath % img_id).getroot()\n",
    "        img = cv2.imread(self._imgpath % img_id)\n",
    "        height, width, channel = img.shape\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target, width, height)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            target = np.array(target)\n",
    "            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n",
    "            img = img[:, :, (2, 1, 0)]  # bgr->rgb\n",
    "            target = np.c_[boxes, np.expand_dims(labels, axis=1)]\n",
    "\n",
    "        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n",
    "\n",
    "    def pull_image(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        # Note: here use the bgr form (rgb is also do well: remember to change mean)\n",
    "        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n",
    "\n",
    "    def pull_anno(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        anno = ET.parse(self._annopath % img_id).getroot()\n",
    "        gt = self.target_transform(anno, 1, 1)  # back original size\n",
    "        return img_id[1], gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnotationTransform(object):\n",
    "    \"\"\"Transforms a VOC annotation into a Tensor of bbox coords and label index\n",
    "    Initilized with a dictionary lookup of classnames to indexes\n",
    "\n",
    "    Arguments:\n",
    "        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_to_ind=None, keep_difficult=False):\n",
    "        self.class_to_ind = class_to_ind or dict(zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "\n",
    "    def __call__(self, target, width, height):\n",
    "        res = []\n",
    "        for obj in target.iter('object'):\n",
    "            difficult = int(obj.find('difficult').text) == 1\n",
    "            if not self.keep_difficult and difficult:\n",
    "                continue\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bbox = obj.find('bndbox')\n",
    "\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            bndbox = []\n",
    "            for i, pt in enumerate(pts):\n",
    "                cur_pt = int(bbox.find(pt).text) - 1\n",
    "                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n",
    "                bndbox.append(cur_pt)\n",
    "            label_idx = self.class_to_ind[name]\n",
    "            bndbox.append(label_idx)\n",
    "            res += [bndbox]  # each elem: [xmin, ymin, xmax, ymax, label_ind]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic transform: norm+scale, mean is bgr form\n",
    "# Note: weights from yolo-official not minus mean but with scale\n",
    "class BaseTransform(object):\n",
    "    def __init__(self, size=300, mean=(104, 117, 123), scale=False):\n",
    "        self.size = size\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image, boxes=None, labels=None):\n",
    "        image = cv2.resize(image, (self.size, self.size)).astype(np.float32)\n",
    "        image -= self.mean\n",
    "        image = image / 255.0 if self.scale else image\n",
    "        return image, boxes, labels\n",
    "\n",
    "\n",
    "def detection_collate(batch):\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets\n",
    "\n",
    "\n",
    "# image: np.array, box: tuple (left, top, right, bottom)\n",
    "def draw_box(image, label, box, c):\n",
    "    h, w = image.shape[:2]\n",
    "    thickness = (w + h) // 300\n",
    "    left, top, right, bottom = box\n",
    "    top, left = max(0, np.round(top).astype('int32')), max(0, np.round(left).astype('int32'))\n",
    "    right, bottom = min(w, np.round(right).astype('int32')), min(h, np.round(bottom).astype('int32'))\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), cfg.colors[c], thickness)\n",
    "    cv2.putText(image, label, (left, top - 5), 0, 0.5, cfg.colors[c], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = AnnotationTransform()\n",
    "root = '/scratch/rw2268/VOCdevkit/'\n",
    "image_set = [('2007', 'trainval')]\n",
    "dataset = VOCDetection(root, image_set, transform=BaseTransform(), target_transform=target_transform)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, BATCH_SIZE, num_workers=4,\n",
    "                         shuffle=True, collate_fn=detection_collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  89.,   90.,   90.,  ..., -123., -120., -112.],\n",
       "          [  89.,   90.,   90.,  ..., -123., -120., -112.],\n",
       "          [  87.,   88.,   89.,  ..., -123., -121., -106.],\n",
       "          ...,\n",
       "          [-103., -108., -108.,  ..., -123., -123., -123.],\n",
       "          [-104., -105., -110.,  ..., -122., -123., -122.],\n",
       "          [-107., -103., -108.,  ..., -122., -123., -122.]],\n",
       " \n",
       "         [[  97.,   98.,   99.,  ..., -117., -114., -105.],\n",
       "          [  98.,   99.,  100.,  ..., -117., -114., -105.],\n",
       "          [  99.,  100.,  100.,  ..., -117., -115.,  -99.],\n",
       "          ...,\n",
       "          [ -85.,  -91.,  -91.,  ..., -117., -117., -117.],\n",
       "          [ -88.,  -89.,  -94.,  ..., -117., -117., -116.],\n",
       "          [ -94.,  -90.,  -92.,  ..., -117., -117., -116.]],\n",
       " \n",
       "         [[ 131.,  132.,  132.,  ..., -104.,  -99.,  -88.],\n",
       "          [ 130.,  130.,  131.,  ..., -104.,  -99.,  -88.],\n",
       "          [ 127.,  129.,  131.,  ..., -104., -102.,  -85.],\n",
       "          ...,\n",
       "          [ -79.,  -85.,  -88.,  ..., -104., -104., -104.],\n",
       "          [ -85.,  -85.,  -91.,  ..., -104., -104., -103.],\n",
       "          [ -89.,  -84.,  -87.,  ..., -104., -104., -103.]]]),\n",
       " array([[ 0.39      ,  0.48955224,  0.976     ,  0.73432836, 18.        ]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5011\n",
      "torch.Size([3, 300, 300])\n",
      "[[0.524      0.56       0.646      0.90133333 8.        ]\n",
      " [0.328      0.70133333 0.504      0.98933333 8.        ]\n",
      " [0.48       0.51466667 0.588      0.79466667 8.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "img, gt = dataset[0]\n",
    "print(img.size())\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SSD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rw2268/1006/detection-pytorch/ssd/utils_ssd/L2Norm.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.weight, self.gamma)\n"
     ]
    }
   ],
   "source": [
    "from ssd.ssd300 import build_ssd\n",
    "net = build_ssd('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_() if m.bias is not None else None\n",
    "\n",
    "\n",
    "# Sets the learning rate to the initial LR decayed by 10 at every specified step\n",
    "def adjust_learning_rate(optimizer, lr, gamma, step):\n",
    "    lr = lr * (gamma ** step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "cuda = True\n",
    "lr = 1e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(VOC_CLASSES) + 1\n",
    "overlap_thresh = 0.5\n",
    "neg_pos = 3\n",
    "variance = [0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd.utils_ssd.box_utils import match, log_sum_exp\n",
    "\n",
    "\n",
    "# evaluate conf_loss and loc_loss\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.variance = variance\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        loc_data, conf_data, priors = preds\n",
    "        num = loc_data.size(0)\n",
    "        num_priors = priors.size(0)\n",
    "        # match priors (priors->nearest target)\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        if loc_data.is_cuda:\n",
    "            loc_t, conf_t = loc_t.cuda(), conf_t.cuda()\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1]\n",
    "            labels = targets[idx][:, -1]\n",
    "            defaults = priors\n",
    "            match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx)\n",
    "        pos = conf_t > 0\n",
    "        # location loss\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "\n",
    "        # evaluate each priors's loss (the same as the paper)\n",
    "        batch_conf = conf_data\n",
    "        loss_c = (log_sum_exp(batch_conf) - batch_conf.gather(2, conf_t.unsqueeze(2))).squeeze(2)\n",
    "        # hard negative mining: note: the batch size of each iteration is not the same\n",
    "        # find the \"max loss\" background\n",
    "        loss_c[pos] = 0  # filter out pos boxes\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio * num_pos, max=pos.size(1) - 1)  # size: [num, 1]\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "        # confidence loss (pos:neg=1:3)\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx + neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weightd = conf_t[(pos + neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weightd, size_average=False)\n",
    "\n",
    "        return loss_l / num_pos.sum().float(), loss_c / num_pos.sum().float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = MultiBoxLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "loc_loss, conf_loss = 0, 0\n",
    "epoch_num = 2\n",
    "step_index = 0\n",
    "epoch_size = len(dataset) // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SSD on VOC\n",
      "Timer: 0.6752 sec.\n",
      "epoch 0 iter 0 || Loss: 15.1683 ||  Timer: 0.5978 sec.\n",
      "epoch 0 iter 10 || Loss: 14.9793 ||  Timer: 0.5953 sec.\n",
      "epoch 0 iter 20 || Loss: 14.9926 ||  Timer: 0.5838 sec.\n",
      "epoch 0 iter 30 || Loss: 14.9264 ||  Timer: 0.5968 sec.\n",
      "epoch 0 iter 40 || Loss: 14.9740 ||  Timer: 0.6005 sec.\n",
      "epoch 0 iter 50 || Loss: 14.9197 ||  Timer: 0.6007 sec.\n",
      "epoch 0 iter 60 || Loss: 14.8166 ||  Timer: 0.6016 sec.\n",
      "epoch 0 iter 70 || Loss: 14.9760 ||  Timer: 0.6043 sec.\n",
      "epoch 0 iter 80 || Loss: 14.8129 ||  Timer: 0.6051 sec.\n",
      "epoch 0 iter 90 || Loss: 14.8660 ||  Timer: 0.6017 sec.\n",
      "epoch 0 iter 100 || Loss: 14.8932 ||  Timer: 0.6196 sec.\n",
      "epoch 0 iter 110 || Loss: 14.8490 ||  Timer: 0.6034 sec.\n",
      "epoch 0 iter 120 || Loss: 14.8722 ||  Timer: 0.6003 sec.\n",
      "epoch 0 iter 130 || Loss: 14.8485 ||  Timer: 0.6187 sec.\n",
      "epoch 0 iter 140 || Loss: 14.9410 ||  Timer: 0.5913 sec.\n",
      "epoch 0 iter 150 || Loss: 14.7774 ||  Timer: 0.6128 sec.\n",
      "epoch 1 iter 0 || Loss: 14.8733 ||  Timer: 0.5939 sec.\n",
      "epoch 1 iter 10 || Loss: 14.8163 ||  Timer: 0.5907 sec.\n",
      "epoch 1 iter 20 || Loss: 14.7942 ||  Timer: 0.6357 sec.\n",
      "epoch 1 iter 30 || Loss: 14.7113 ||  Timer: 0.6225 sec.\n",
      "epoch 1 iter 40 || Loss: 14.8121 ||  Timer: 0.5916 sec.\n",
      "epoch 1 iter 50 || Loss: 14.7366 ||  Timer: 0.6140 sec.\n",
      "epoch 1 iter 60 || Loss: 14.8714 ||  Timer: 0.5851 sec.\n",
      "epoch 1 iter 70 || Loss: 14.7303 ||  Timer: 0.6349 sec.\n",
      "epoch 1 iter 80 || Loss: 14.6886 ||  Timer: 0.5898 sec.\n",
      "epoch 1 iter 90 || Loss: 14.7622 ||  Timer: 0.5930 sec.\n",
      "epoch 1 iter 100 || Loss: 14.6588 ||  Timer: 0.5929 sec.\n",
      "epoch 1 iter 110 || Loss: 14.7923 ||  Timer: 0.5923 sec.\n",
      "epoch 1 iter 120 || Loss: 14.9608 ||  Timer: 0.5895 sec.\n",
      "epoch 1 iter 130 || Loss: 14.8561 ||  Timer: 0.6350 sec.\n",
      "epoch 1 iter 140 || Loss: 14.7275 ||  Timer: 0.5939 sec.\n",
      "epoch 1 iter 150 || Loss: 14.6644 ||  Timer: 0.6223 sec.\n",
      "epoch 2 iter 0 || Loss: 14.6399 ||  Timer: 0.5915 sec.\n",
      "epoch 2 iter 10 || Loss: 14.8059 ||  Timer: 0.5889 sec.\n",
      "epoch 2 iter 20 || Loss: 14.6808 ||  Timer: 0.5888 sec.\n",
      "epoch 2 iter 30 || Loss: 14.7779 ||  Timer: 0.5999 sec.\n",
      "epoch 2 iter 40 || Loss: 14.6370 ||  Timer: 0.6127 sec.\n",
      "epoch 2 iter 50 || Loss: 14.7730 ||  Timer: 0.5939 sec.\n",
      "epoch 2 iter 60 || Loss: 14.9270 ||  Timer: 0.6029 sec.\n",
      "epoch 2 iter 70 || Loss: 14.6662 ||  Timer: 0.6388 sec.\n",
      "epoch 2 iter 80 || Loss: 14.8728 ||  Timer: 0.5881 sec.\n",
      "epoch 2 iter 90 || Loss: 14.6702 ||  Timer: 0.6057 sec.\n",
      "epoch 2 iter 100 || Loss: 14.8248 ||  Timer: 0.6006 sec.\n",
      "epoch 2 iter 110 || Loss: 14.7908 ||  Timer: 0.5908 sec.\n",
      "epoch 2 iter 120 || Loss: 14.7402 ||  Timer: 0.6373 sec.\n",
      "epoch 2 iter 130 || Loss: 14.8681 ||  Timer: 0.5883 sec.\n",
      "epoch 2 iter 140 || Loss: 14.8922 ||  Timer: 0.5910 sec.\n",
      "epoch 2 iter 150 || Loss: 14.8522 ||  Timer: 0.6092 sec.\n",
      "epoch 3 iter 0 || Loss: 14.8824 ||  Timer: 0.5919 sec.\n",
      "epoch 3 iter 10 || Loss: 14.8249 ||  Timer: 0.5992 sec.\n",
      "epoch 3 iter 20 || Loss: 14.7258 ||  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-33:\n",
      "Process Process-35:\n",
      "Process Process-36:\n",
      "Process Process-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2af1facd3eb8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/rw2268/.conda/envs/nlp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 201110) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-bf26d3b0be30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_c\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training SSD on VOC\")\n",
    "\n",
    "batch_iterator = None\n",
    "images = torch.randn((BATCH_SIZE, 3, 300, 300), requires_grad=True)\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "    images = images.cuda()\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    # to do: adjust learning rate\n",
    "    for i, (imgs, targets) in enumerate(data_loader):\n",
    "        if i == epoch_size:\n",
    "            break\n",
    "        images.data.copy_(imgs)\n",
    "        targets = [anno.cuda() for anno in targets] if cuda else [anno for anno in targets]\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c = criterion(out, targets)\n",
    "        loss = loss_c + loss_l\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        if i % 10 == 0:\n",
    "            print('Timer: %.4f sec.' % (t1 - t0))\n",
    "            print('epoch ' + repr(epoch) + ' iter ' + repr(i) + ' || Loss: %.4f || ' % (loss.item()), end=' ')\n",
    "#     if epoch % 20 == 0 and epoch != 0:\n",
    "#         print('Saving state, epoch: ', epoch)\n",
    "#         torch.save(net.state_dict(), '../weights/ssd/ssd300_' + repr(epoch) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
